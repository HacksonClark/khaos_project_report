@inproceedings {Dean:2009,
  author = {Jeff Dean},
  title = "{Designs, Lessons and Advice from Building Large
Distributed Systems}",
  booktitle = {Proceedings of the the 3rd Large Scale Distributed Systems and Middleware (LADIS'09)},
  year = {2009},
  month = OCT,
}
@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{chaosmesh2023,
  title={Chaos Mesh: Cloud-native Chaos Engineering Platform},
  author={PingCAP},
  howpublished={\url{https://chaos-mesh.org/}},
  year={2023},
  note={Accessed: 2025-04-17}
}

@misc{netflixchaosmonkey,
  title={Netflix Chaos Monkey},
  author={Netflix},
  howpublished={\url{https://netflix.github.io/chaosmonkey/}},
  note={Accessed: 2025-04-17}
}

@inproceedings{sassifi2017,
  title={SASSIFI: Evaluating resilience of GPU applications using fault injection},
  author={Li, Jia and Sun, Guojin and Tan, Lin and Lu, Shan},
  booktitle={Proceedings of the 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year={2017},
  organization={IEEE}
}

@inproceedings{tensorfi2022,
  title={TensorFI+: Injection-based resilience evaluation for TensorFlow models},
  author={Chen, Wei and Roy, Kaushik and Raghunathan, Anand},
  booktitle={Proceedings of the 2022 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  year={2022},
  organization={IEEE}
}

@inproceedings{mutiny2024,
  title={Mutiny in the Cluster: Bit Flip Chaos in etcd’s Persistent State},
  author={Barletta, Alessandro and Venkataramani, Arun and Alipourfard, Omid and others},
  booktitle={Proceedings of the 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2024},
  organization={USENIX}
}

@misc{llama3herdmodels,
  title={Llama 3 and the Herd of GPUs},
  author={Vigraham, Subbu and Leonhardi, Martin and others},
  howpublished={\url{https://ai.meta.com/research/publications/llama-3-herd-models/}},
  year={2024},
  note={Accessed: 2025-04-17}
}

@article{facebookdramstudy,
  title={DRAM Errors in the Wild: A Large-Scale Field Study},
  author={Schroeder, Bianca and Pinheiro, Eduardo},
  journal={Communications of the ACM},
  volume={54},
  number={2},
  pages={100--107},
  year={2011},
  publisher={ACM}
}

@inproceedings{facebookssdstudy,
  title={Flash reliability in production: The expected and the unexpected},
  author={Meza, Justin and Kumar, Qiang and Mutlu, Onur and Gniady, Chris},
  booktitle={Proceedings of the 11th USENIX Conference on File and Storage Technologies (FAST)},
  year={2015},
  organization={USENIX}
}

@inproceedings{filibuster2024,
  title={Filibuster: Fault Injection for Resilience Testing of Distributed Systems},
  author={Wang, Huan and Jung, Jungwoo and Zhao, Yuchen and Ernst, Michael D.},
  booktitle={Proceedings of the 46th International Conference on Software Engineering (ICSE)},
  year={2024},
  organization={IEEE}
}

@inproceedings{zhangelbaum2012,
  title={Amplifying tests to validate exception handling code},
  author={Zhang, Xusheng and Elbaum, Sebastian},
  booktitle={Proceedings of the 2012 International Symposium on Software Testing and Analysis (ISSTA)},
  pages={171--181},
  year={2012},
  organization={ACM}
}

@inproceedings{Wang2017,
  author = {Guosai Wang and Lifei Zhang and Wei Xu},
  title = {What Can We Learn from Four Years of Data Center Hardware Failures?},
  booktitle = {Proc.\ 47th Annu.\ IEEE/IFIP Int.\ Conf.\ on Dependable Systems and Networks (DSN)},
  year = {2017},
  pages = {25--36},
  doi = {10.1109/DSN.2017.26}
}

@inproceedings{Hochschild2021,
  author = {Peter H. Hochschild and Paul J. Turner and Jeffrey C. Mogul and Rama K. Govindaraju and Parthasarathy Ranganathan and David E. Culler and Amin Vahdat},
  title = {Cores that don't count},
  booktitle = {Proc.\ 18th Workshop on Hot Topics in Operating Systems (HotOS XVIII)},
  year = {2021}
}

@misc{Dixit2021,
  author = {Harish D. Dixit and Sneha Pendharkar and Matt Beadon and Chris Mason and Tejasvi Chakravarthy and Bharath Muthiah and Sriram Sankar},
  title = {Silent Data Corruptions at Scale},
  howpublished = {arXiv:2102.11245},
  year = {2021}
}

@misc{Zhang2022opt,
  author = {Susan Zhang et al.},
  title = {{OPT}: Open Pre-trained Transformer Language Models},
  howpublished = {Meta AI Technical Report (arXiv:2205.01068)},
  year = {2022}
}

@article{Basiri2016,
  author = {Ali Basiri and Niosha Behnam and Ruud de Rooij and Lorin Hochstein and Luke Kosewski and Justin Reynolds and Casey Rosenthal},
  title = {Chaos Engineering},
  journal = {IEEE Software},
  volume = {33},
  number = {3},
  pages = {35--41},
  year = {2016},
  doi = {10.1109/MS.2016.60}
}

@misc{Zhang2020phoebe,
  author = {Long Zhang and Brice Morin and Benoit Baudry and Martin Monperrus},
  title = {Maximizing Error Injection Realism for Chaos Engineering with System Calls (PHOEBE)},
  howpublished = {arXiv:2006.04444},
  year = {2020}
}

@techreport{intel2023mca,
  title        = {{MCA/MFP Helps JD Stable and Efficient Cloud Services}},
  author       = {{Intel Corporation}},
  institution  = {Intel Corporation},
  type         = {White Paper},
  number       = {Document Number: 2023-12},
  year         = {2023},
  month        = dec,
  url          = {https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2023-12/mca-mfp-helps-jd-stable-and-efficient-cloud-services.pdf},
  note         = {Accessed: 2025-05-06}
}

@inproceedings{oobleck,
    author = {Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf},
    title = {Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates},
    year = {2023},
    isbn = {9798400702297},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3600006.3613152},
    doi = {10.1145/3600006.3613152},
    abstract = {Oobleck enables resilient distributed training of large DNN models with guaranteed fault tolerance. It takes a planning-execution co-design approach, where it first generates a set of heterogeneous pipeline templates and instantiates at least f + 1 logically equivalent pipeline replicas to tolerate any f simultaneous failures. During execution, it relies on already-replicated model states across the replicas to provide fast recovery. Oobleck provably guarantees that some combination of the initially created pipeline templates can be used to cover all available resources after f or fewer simultaneous failures, thereby avoiding resource idling at all times. Evaluation on large DNN models with billions of parameters shows that Oobleck provides consistently high throughput, and it outperforms state-of-the-art fault tolerance solutions like Bamboo and Varuna by up to 13.9\texttimes{}.},
    booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
    pages = {382–395},
    numpages = {14},
    keywords = {pipeline template, hybrid parallelism, distributed training, fault tolerant training},
    location = {<conf-loc>, <city>Koblenz</city>, <country>Germany</country>, </conf-loc>},
    series = {SOSP '23}
}

@misc{wd_2024_hdd,
  author = {Western Digital},
  title = {A Balancing Act: HDDs and SSDs in Modern Data Centers},
  year = {2024},
  url = {https://blog.westerndigital.com/a-balancing-act-hdds-and-ssds-in-modern-data-centers/},
  urldate = {2025-05-07}
}

@inproceedings{sridharan2015mem,
author = {Sridharan, Vilas and DeBardeleben, Nathan and Blanchard, Sean and Ferreira, Kurt B. and Stearley, Jon and Shalf, John and Gurumurthi, Sudhanva},
title = {Memory Errors in Modern Systems: The Good, The Bad, and The Ugly},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694348},
doi = {10.1145/2694344.2694348},
abstract = {Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several key findings about the efficacy of many currently deployed reliability techniques such as DRAM ECC, DDR address/command parity, and SRAM ECC and parity. We also perform a methodological study, and find that counting errors instead of faults, a common practice among researchers and data center operators, can lead to incorrect conclusions about system reliability. Finally, we use our data to project the needs of future large-scale systems. We find that SRAM faults are unlikely to pose a significantly larger reliability threat in the future, while DRAM faults will be a major concern and stronger DRAM resilience schemes will be needed to maintain acceptable failure rates similar to those found on today's systems.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {297–310},
numpages = {14},
keywords = {reliability, large-scale systems, field studies},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@INPROCEEDINGS{utrera2018hpc,
  author={Utrera, Gladys and Gil, Marisa and Martorell, Xavier},
  booktitle={2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)}, 
  title={Analysis of the Impact Factors on Data Error Propagation in HPC Applications}, 
  year={2018},
  volume={},
  number={},
  pages={546-549},
  keywords={Benchmark testing;Software;Libraries;Toy manufacturing industry;Runtime;Sparse matrices;Computer architecture;data error propagation;reliability;MPI resilience},
  doi={10.1109/PDP2018.2018.00092}}