\section{Introduction}
\label{sec:introduction}

Hardware failures are commonplace in data centers. For example, a new cluster at Google is expected to undergo a thousand machine failures and thousands of disk failures every year \cite{Dean:2009}. Cloud data center systems increasingly face the reality that hardware faults are the norm rather than the exception. Large-scale field studies have shown that hardware failures occur frequently in production environments and can severely impact service reliability and revenue~\cite{Wang2017}. For example, an analysis of dozens of data centers recorded over 290{,}000 hardware failure incidents in just four years~\cite{Wang2017}. Such failures span a wide range of types, including transient CPU bit flips, disk errors, and network faults. Notably, some failures manifest in subtle and silent ways: Google engineers recently documented “mercurial” CPU cores that intermittently produce incorrect results without any visible signal~\cite{Hochschild2021}. Similarly, Facebook identified hundreds of CPUs exhibiting silent data corruption, including errors like returning $5$ for the computation $2 \times 3$~\cite{Dixit2021}. These rare but dangerous behaviors become inevitable at scale, especially in massive LLM training clusters~\cite{Zhang2022opt}.

Despite the known risk, testing software systems against hardware failures remains notoriously difficult. The space of possible faults is vast—ranging from memory errors and I/O issues to network partitions—and developers lack generic tools to simulate them in realistic settings. Existing fault injection systems tend to be coarse-grained or platform-specific. For example, Netflix's Chaos Monkey popularized chaos engineering as a discipline that introduces random disruptions (e.g., killing VMs or containers) to uncover systemic weaknesses~\cite{Basiri2016}. While effective for macro-level resilience testing, such approaches do not simulate low-level hardware malfunctions such as disk read/write errors or memory allocation failures.

\textbf{Khaos} fills this gap by providing a lightweight, extensible framework for fine-grained fault injection using eBPF (Extended Berkeley Packet Filter). eBPF is a modern Linux kernel technology that enables sandboxed programs to run in response to kernel events such as system calls. By attaching to specific syscall probes (e.g., \texttt{write()}, \texttt{mmap()}, \texttt{ioctl()}), Khaos can simulate hardware-like behavior (e.g., disk full, out-of-memory, I/O errors) by using a helper function named \texttt{bpf\_override\_return()} to inject custom error codes. The injected faults appear to user-space programs just like real hardware failures, enabling realistic testing with no need to modify applications or kernel code.

Our approach builds on recent research showing the effectiveness of syscall-level fault injection~\cite{Zhang2020phoebe}. Khaos enables fault injection by process ID, supports dozens of failure modes, and offers a clean abstraction for adding new ones. We believe it complements existing macro-level chaos tools and helps developers harden systems against elusive and costly hardware problems. As cloud-scale systems grow in complexity, tools like Khaos are essential for building fault-tolerant infrastructure from the ground up.
